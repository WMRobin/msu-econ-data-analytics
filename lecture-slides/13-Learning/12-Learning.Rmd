---
title: "Topic 12: <br> Machine Learning Fundamentals"
author: "Nick Hagerty* <br> ECNS 460/560 Fall 2023 <br> Montana State University"
date: "<br> .small[*Adapted from [“Prediction and machine-learning in econometrics”](https://github.com/edrubin/EC524W21) by Ed Rubin, used with permission. Other than the simulation in slides 20-28 (which is included), these slides are excluded from this resource's overall CC license.]"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

name: toc

```{css, echo=FALSE}
.text-small {
  font-size: 50%;
}
.text-big {
  font-size: 150%;
}
.small {
  font-size: 75%;
}
```

```{R, setup, include = F}
library(pacman)
p_load(
  broom, tidyverse,
  ggplot2, ggthemes, ggforce, ggridges, cowplot, scales,
  latex2exp, viridis, extrafont, gridExtra, plotly, ggformula,
  kableExtra, snakecase, janitor, DT,
  data.table, dplyr,
  lubridate, knitr, future, furrr, parallel,
  MASS, estimatr, FNN, parsnip, caret, glmnet,
  huxtable, here, magrittr, 
  patchwork, splines
)
# Define colors
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#3b3b9a"
green      = "#8bb174"
grey_light = "grey70"
grey_mid   = "grey50"
grey_dark  = "grey20"
purple     = "#6A5ACD"
slate      = "#314f4f"
# Knitr options
opts_chunk$set(
  comment = "#>",
	cache = TRUE,
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F
)
opts_chunk$set(dev = "svg")
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height = height)
})
options(knitr.table.format = "html")
```


# Table of contents

1. [Overview: Statistical learning](#learning)

1. [Assessing model accuracy](#accuracy)

1. [Cross-validation](#resampling)


---
class: inverse, middle
name: learning
# Overview: Statistical learning

<!-- --- -->
<!-- # Review: Goals of data analysis -->

<!-- There are **3 main purposes** of data analysis: -->
<!-- 1. .hi-turquoise[**Descriptive analysis:**] Characterize observed patterns among variables. -->
<!-- 2. .hi[**Causal inference:**] Learn how Y changes as a result of an active intervention to change X. -->
<!-- 3. .hi-purple[**Prediction:**] Predict the value of one variable from other information. -->

<!-- Key differences among the 3 goals: -->
<!-- 1. **Focus:** Prediction focuses on the outcome $\color{#6A5ACD}{\hat{Y_{i}}}$, causal inference on a coefficient $\color{#e64173}{\hat{\beta_0}}$. -->
<!-- 2. **Selection bias** is a problem for causal inference, but not predictive or descriptive analysis. -->
<!-- 3. **Interpretion:** Coefficients have meaning in causal inference and descriptive analysis, but little meaning in prediction. -->


---
# Prediction

$$\color{#6A5ACD}{Y_i} = f(x_{0i}, x_{1i}, ..., x_{Ni}) = \color{#e64173}{\beta_0} x_{0i} + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \varepsilon_i$$

.hi-purple[**Prediction:**] Want to estimate $\color{#6A5ACD}{\hat{Y_{i}}}$ given observed data $x_{0i}, x_{1i}, x_{2i}, ....$.
- Does not matter whether your model is "correct" (the true DGP), only whether it *works.*

--

The idea is that we will:
1. **Train** a model on data for which we know both $X$ and $Y$.
2. **Apply** the model to new situations where we know $X$ but not $Y$.


---
layout: true
# Statistical learning

---

The class of methods for doing prediction is called **statistical learning** or **machine learning.**

First, a few definitions...


---
## Supervised vs. unsupervised

Statistical learning is generally divided into two classes:

1. .hi-slate[Supervised learning] builds ("learns") a statistical model for predicting an .hi-orange[output] $\left( \color{#FFA500}{\mathbf{y}} \right)$ given a set of .hi-purple[inputs] $\left( \color{#6A5ACD}{x_{1},\, \ldots,\, x_{p}} \right)$.

--

2. .hi-slate[Unsupervised learning] learns relationships and structure using only .hi-purple[inputs] $\left( \color{#6A5ACD}{x_{1},\, \ldots,\, x_{p}} \right)$ without any *supervising* output — letting the data "speak for itself."

---
class: clear, middle
layout: false

```{r, comic, echo = F}
knitr::include_graphics("images/comic-learning.jpg")
```

.it[.smaller[[Source](https://twitter.com/athena_schools/status/1063013435779223553)]]

---
layout: true
# Statistical learning

---
## Classification vs. Regression

.hi-slate[Supervised learning] is broken into two types, based on what kind of .hi-orange[output] we want to predict:

1. .hi-slate[Classification tasks] for which the values of $\color{#FFA500}{\mathbf{y}}$ are discrete categories.
<br>*E.g.*, race, sex, loan default, hazard, disease, flight status

2. .hi-slate[Regression tasks] in which $\color{#FFA500}{\mathbf{y}}$ takes on continuous, numeric values.
<br>*E.g.*, price, arrival time, number of emails, temperature

.note[Note] The use of .it[regression] differs from our use of .it[linear regression].

---
## Translating terms

$\color{#6A5ACD}{\mathbf{X}}$ (treatment variable/covariates, independent variables, regressors)
- Now: **predictors, features.**

$\hat{\color{#FFA500}{\mathbf{Y}}}$ (outcome variable, dependent variable)
- Now: **target, label.**

"Estimate a model" or "fit a model"
- Now: **Train** a model, **learn** a model.


---
## Prediction errors

Imagine there is a .turquoise[function] $\color{#20B2AA}{f}$ that takes .purple[inputs] $\color{#6A5ACD}{\mathbf{X}} = \color{#6A5ACD}{\mathbf{x}_1}, \ldots, \color{#6A5ACD}{\mathbf{x}_p}$ <br>and maps them, plus a random, mean-zero .pink[error term] $\color{#e64173}{\varepsilon}$, to the .orange[output].
$$\color{#FFA500}{\mathbf{y}} = \color{#20B2AA}{f} \! \left( \color{#6A5ACD}{\mathbf{X}} \right) + \color{#e64173}{\varepsilon}$$

The accuracy of $\hat{\color{#FFA500}{\mathbf{y}}}$ depends upon .hi-slate[two errors]:

1. .hi-slate[Reducible error] The error due to $\hat{\color{#20B2AA}{f}}$ imperfectly estimating $\color{#20B2AA}{f}$.
<br>*Reducible* in the sense that we can improve $\hat{\color{#20B2AA}{f}}$.

1. .hi-slate[Irreducible error] The error component that is outside of the model $\color{#20B2AA}{f}$.
<br>*Irreducible* because we defined an error term $\color{#e64173}{\varepsilon}$ unexplained by $\color{#20B2AA}{f}$.

Our goal is to minimize .hi-slate[reducible error].


---
## How to predict

**How do we minimize reducible error** (and form the best predictions)?

The basic workflow for predictive analysis:

1. **Choose** a model.
2. **Train** the model (estimate its parameters).
3. **Assess** its performance.
4. Repeat steps 1-3 for different models, and choose the **best** model.

--

.note[Note:] "Different models" can mean:

* Either completely distinct models (e.g., OLS vs. *k*-nearest neighbors).
* Or a set of closely related models differing by a **hyperparameter**.
  - E.g., polynomial regression (hyperparameter: polynomial degree).


---
## How to predict

**How do we minimize reducible error** (and form the best predictions)?

The basic workflow for predictive analysis:

1. **Choose** a model.
2. **Train** the model (estimate its parameters).
3. **Assess** its performance.
4. Repeat steps 1-3 for different models, and choose the **best** model.

.qa[Q] Why don't we do this for causal inference?
- In causal inference, the best model is determined by outside knowledge (i.e., which research design is most plausible).
- In prediction, the best model is simply the one that performs best.


---
layout: false
class: inverse, middle
name: accuracy

# Assessing model accuracy


---
layout: true
# Model accuracy

---
name: accuracy-subtlety

## Subtlety

Defining performance can be tricky...

*Regression:*

- Which do you prefer?
  1. Lots of little errors and a few really large errors.
  1. Medium-sized errors for everyone.

- Is a 1-unit error (*e.g.*, $1,000) equally bad for everyone?

- Is an overestimate equally bad as an underestimate?

---
## Subtlety

Defining performance can be tricky...

*Classification:*

- Which is worse?
  1. False positive (*e.g.*, incorrectly diagnosing cancer)
  1. False negative (*e.g.*, missing cancer)

- Which is more important?
  1. True positive (*e.g.*, correct diagnosis of cancer)
  1. True negative (*e.g.*, correct diagnosis of "no cancer")

---
## Loss

*Prediction error* is defined as:
$$\color{#FFA500}{\mathbf{y}}_i - \hat{\color{#20B2AA}{f}}\!\left( \color{#6A5ACD}{x}_i \right) = \color{#FFA500}{\mathbf{y}}_i - \hat{\color{#FFA500}{\mathbf{y}}}_i$$
the difference between the label $\left( \color{#FFA500}{\mathbf{y}} \right)$ and its prediction $\left( \hat{\color{#FFA500}{\mathbf{y}}} \right)$.

The distance (_i.e._, non-negative value) between a true value and its prediction is often called .b[loss].

The way you choose to use loss to measure model performance is called a **loss function.**

---
name: mse
## MSE

.attn[Mean squared error (MSE)] is the most common.super[.pink[†]] loss function in a regression setting.

.footnote[
.pink[†] *Most common* does not mean best—it just means lots of people use it.
]

$$\text{MSE} = \dfrac{1}{n} \sum_{i=1}^n \left[ \color{#FFA500}{y}_i - \hat{\color{#20B2AA}{f}}(\color{#6A5ACD}{x}_i) \right]^2$$

--

Note:

1. MSE is small when .hi-slate[prediction error] is small.
1. MSE .hi-slate[penalizes] big errors more than small errors (the squared part).

---
name: overfitting
layout: false
# Model accuracy
## Overfitting

Low MSE (accurate performance) on the data that trained the model is not necessarily impressive — maybe the model is just overfitting our data.

More flexible models...

- might better fit complex systems.

- but also might falsely interpret noise as signal.

---
name: training-testing

# Model accuracy
## Training or testing?

.note[What we want:] How well does the model perform .hi-slate[on data it has never seen]?

--

This introduces an important distinction:

1. .hi-slate[Training data]: The observations $(\color{#FFA500}{y}_i,\color{#e64173}{x}_i)$ used to .hi-slate[train] our model $\hat{\color{#20B2AA}{f}}$.
1. .hi-slate[Testing data]: The observations $(\color{#FFA500}{y}_0,\color{#e64173}{x}_0)$ that our model has yet to see—and which we can use to evaluate the performance of $\hat{\color{#20B2AA}{f}}$.

--

.hi-slate[Real goal: Low test-set MSE] (not the training-set MSE).

.note[Note:] This is different from causal inference, where:
- We don't care about model *performance.*
- We just want to use all available data to learn the model *structure.*


---
class: clear, middle
layout: true

```{r, echo = F, cache = T, eval = T}
# Function to generate our data
sim_fun = function(x) (x - 3)^2 * (x + 3) * (x + 5) / 100 + 7
# Generate data
set.seed(1234)
nnn = 75
sd = 1
train = tibble(
  #x = runif(n = nnn, min = -4.25, max = 4.25),
  x = seq(from = -4.25, to = 4.25, length.out = nnn),
  y = sim_fun(x) + rnorm(nnn, sd = sd)
)
test = tibble(
  #x = runif(n = nnn, min = -4.25, max = 4.25),
  x = seq(from = -4.25, to = 4.25, length.out = nnn),
  y = sim_fun(x) + rnorm(nnn, sd = sd)
)
range = tibble(x = seq(from = -4.25, to = 4.25, length.out = 500))
true_shape = cbind(range, y = sim_fun(range$x))
degrees = c(1, 2, 3, 4, 5, 7, 11, 17, 25)

# Calculate training and test MSEs for each polynomial degree
mses = lapply(
  X = degrees,
  FUN = function(degree) {
    # Fit polynomial on training data
    fit = lm(y ~ poly(x, degree = degree), train)
    # Calculate MSE
    mse_train = (train$y - predict(fit, train))^2 |> mean()
    mse_test = (test$y - predict(fit, test))^2 |> mean()
    # Return data frame
    tibble(
      degree = rep(degree, 2),
      mse_type = c("Train", "Test"),
      mse_value = c(mse_train, mse_test)
    )
  }
) %>% bind_rows()

# Get y-scale mins and maxes
fit = lm(y ~ poly(x, degree = 25), train)
ymax = max(max(predict(fit, range)), max(train$y), max(test$y))
ymin = min(min(predict(fit, range)), min(train$y), min(test$y))

# Function to make plot for one polynomial degree
plot_test_train = function(degree) {
  fit = lm(y ~ poly(x, degree = degree), train)
  predict_shape = cbind(range, yhat = predict(fit, range))
  predict_train = tibble(x = train$x, y = predict(fit, train), type = "hat")
  predict_test = tibble(x = test$x, y = predict(fit, test), type = "hat")
  df_train = bind_rows(cbind(train, type = "raw"), predict_train)
  df_test = bind_rows(cbind(test, type = "raw"), predict_test)
  plot_train = ggplot(df_train) +
    geom_line(aes(x, y, group = x), col = "gray") +
    geom_line(data = predict_shape, aes(x, yhat), color = "blue") + 
    geom_line(data = true_shape, aes(x, y), linetype = "dashed") +
    geom_point(data = df_train |> filter(type == "raw"), aes(x, y), shape = "circle filled", fill = "#00BFC4") +
    ylim(ymin, ymax) +
    theme_void() +
    labs(title = "Training data")
  plot_test = ggplot(df_test) +
    geom_line(aes(x, y, group = x), col = "gray") +
    geom_line(data = predict_shape, aes(x, yhat), color = "blue") + 
    geom_line(data = true_shape, aes(x, y), linetype = "dashed") +
    geom_point(data = df_test |> filter(type == "raw"), aes(x, y), shape = "circle filled", fill = "#F8766D") +
    ylim(ymin, ymax) +
    theme_void() +
    labs(title = "Testing data")
  return(plot_train + plot_test)
}

# Function to make MSE plot up to one polynomial degree
plot_mses = function(d) {
  ggplot(
    mses |> filter(degree <= d), 
    aes(x = degree, y = mse_value, color = mse_type)
    ) +
    geom_line(size = 1) + 
    geom_point(size = 3) + 
    ylim(min(mses$mse_value), max(mses$mse_value)) +
    theme_minimal() +
    theme(axis.text.y = element_blank(), 
          legend.title = element_blank(), 
          legend.position = "top", # text = element_text(size=14)
          ) +
    scale_x_log10(breaks = c(1, 5, 10, 20), minor_breaks = c(1, 2, 3, 4, 5, 10, 15, 20, 25), limits = c(1, 25)) +
    labs(title = "MSE") + ylab("") + xlab("Degree")
}

```

---

Fitting a polynomial of degree **1**
```{r, message = F, echo = F, cache = T, eval = T, fig.width = 8.75, fig.height = 6.25}
d = 1
plot_test_train(d) + plot_mses(d) + plot_layout(widths = c(2, 2, 1))
```

---

Fitting a polynomial of degree **2**
```{r, message = F, echo = F, cache = T, eval = T, fig.width = 8.75, fig.height = 6.25}
d = 2
plot_test_train(d) + plot_mses(d) + plot_layout(widths = c(2, 2, 1))
```

---

Fitting a polynomial of degree **3**
```{r, message = F, echo = F, cache = T, eval = T, fig.width = 8.75, fig.height = 6.25}
d = 3
plot_test_train(d) + plot_mses(d) + plot_layout(widths = c(2, 2, 1))
```

---

Fitting a polynomial of degree **4**
```{r, message = F, echo = F, cache = T, eval = T, fig.width = 8.75, fig.height = 6.25}
d = 4
plot_test_train(d) + plot_mses(d) + plot_layout(widths = c(2, 2, 1))
```

---

Fitting a polynomial of degree **5**
```{r, message = F, echo = F, cache = T, eval = T, fig.width = 8.75, fig.height = 6.25}
d = 5
plot_test_train(d) + plot_mses(d) + plot_layout(widths = c(2, 2, 1))
```

---

Fitting a polynomial of degree **7**
```{r, message = F, echo = F, cache = T, eval = T, fig.width = 8.75, fig.height = 6.25}
d = 7
plot_test_train(d) + plot_mses(d) + plot_layout(widths = c(2, 2, 1))
```

---

Fitting a polynomial of degree **11**
```{r, message = F, echo = F, cache = T, eval = T, fig.width = 8.75, fig.height = 6.25}
d = 11
plot_test_train(d) + plot_mses(d) + plot_layout(widths = c(2, 2, 1))
```

---

Fitting a polynomial of degree **17**
```{r, message = F, echo = F, cache = T, eval = T, fig.width = 8.75, fig.height = 6.25}
d = 17
plot_test_train(d) + plot_mses(d) + plot_layout(widths = c(2, 2, 1))
```

---

Fitting a polynomial of degree **25**
```{r, message = F, echo = F, cache = T, eval = T, fig.width = 8.75, fig.height = 6.25}
d = 25
plot_test_train(d) + plot_mses(d) + plot_layout(widths = c(2, 2, 1))
```

---
layout: false
name: bias-variance
# Model accuracy
## The bias-variance tradeoff

Finding the optimal level of flexibility highlights the .hi-pink[bias]-.hi-purple[variance] .b[tradeoff.]

.hi-pink[Bias:] The error that comes from modeling $\color{#20B2AA}{f}$ with the wrong structure.
- More flexible models are better equipped to recover complex relationships $\left( \color{#20B2AA}{f} \right)$, reducing bias.
- Models that are too simple have high bias.

.hi-purple[Variance:] The amount $\hat{\color{#20B2AA}{f}}$ would change with a different .hi-slate[training sample]
- If new .hi-slate[training sets] drastically change $\hat{\color{#20B2AA}{f}}$, then we have a lot of uncertainty about $\color{#20B2AA}{f}$.
- Models that are too flexible have high variance.

---
# Model accuracy
## The bias-variance tradeoff

The expected value of the .hi-pink[test MSE] can be written
$$
\begin{align}
  \mathop{E}\left[ \left(\color{#FFA500}{\mathbf{y_0}} - \mathop{\hat{\color{#20B2AA}{f}}}\left(\color{#6A5ACD}{\mathbf{X}_0}\right) \right)^2 \right] =
  \underbrace{\mathop{\text{Var}} \left( \mathop{\hat{\color{#20B2AA}{f}}}\left(\color{#6A5ACD}{\mathbf{X}_0}\right) \right)}_{\text{Variance}} +
  \underbrace{\left[ \text{Bias}\left( \mathop{\hat{\color{#20B2AA}{f}}}\left(\color{#6A5ACD}{\mathbf{X}_0}\right) \right) \right]^2}_{\text{Bias}} +
  \underbrace{\mathop{\text{Var}} \left( \varepsilon \right)}_{\text{Irr. error}}
\end{align}
$$

.b[The tradeoff] in terms of model flexibility:

- At first, adding flexibility reduces bias more than it increases variance.
- Later on, the bias reduction gets swamped out by increases in variance.
- At some point, the marginal benefits of flexibility equal marginal costs.


---
class: inverse, middle
name: resampling
# Cross-validation

---
layout: false
# Resampling methods

If we just need to **train** a model once and then **evaluate** its performance...
- We can use the training set to fit the model...
- And then the test set to calculate MSE and see how it did.

--

But what if we also need to **choose** a model / **tune** its parameter(s)?
- We could estimate the **test MSE** for each model we're considering, and then choose the best one.

--

But wait -- now, how do we **evaluate** the model we've chosen?
- We already used up the test set during the training process!
- We chose the model based on this sample, so of course its MSE will be low (too low).

---
layout: false
# Resampling methods

Instead, we can use .hi[resampling methods] to conduct model selection entirely within the training set.
- Hold out a mini "test" sample of the training data that we use to *estimate* the test error.

--

The process:

1. **Fit your model(s)** on only one part of the training data (a sample).
2. **Estimate the test MSE** using the other part of the training data.
3. **Repeat steps 1-2** for different random splits of the training data.
4. **Select a model** based on your estimated test MSE.
5. Finally, **evaluate your chosen model** by estimating MSE in the *real* test set. (Don't touch it until the very end!)


---
name: resampling-validation
layout: true
# Resampling methods
## Option 1: The .it[validation set] approach

To estimate the .hi-pink[test error], we can .it[hold out] a subset of our .hi-purple[training data] and then .hi-slate[validate] (evaluate) our model on this held out .hi-slate[validation set].

- The .hi-slate[validation error rate] estimates the .hi-pink[test error rate]
- The model only "sees" the non-validation subset of the .hi-purple[training data].

---

```{r, data-validation-set, include = F, cache = T}
# Generate data
X = 40
Y = 12
set.seed(12345)
v_df = expand_grid(
  x = 1:X,
  y = 1:Y
) %>% mutate(grp = sample(
  x = c("Train", "Validate"),
  size = X * Y,
  replace = T,
  prob = c(0.7, 0.3)
)) %>% mutate(
  grp2 = c(
    rep("Validate", sum(grp == "Validate")),
    rep("Train", sum(grp == "Train"))
  )
)
```

---

```{r, plot-validation-set, echo = F, dependson = "data-validation-set", fig.height = 3, cache = T}
ggplot(data = v_df, aes(x, y, fill = grp, color = grp)) +
geom_point(shape = 21, size = 4.5, stroke = 0.5, color = purple, fill = "white") +
theme_void() +
theme(legend.position = "none")
```

.col-left[.hi-purple[Initial training set]]


---

```{r, plot-validation-set-2, echo = F, dependson = "data-validation-set", fig.height = 3, cache = T}
ggplot(data = v_df, aes(x, y, fill = grp, color = grp)) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", slate)) +
scale_color_manual("", values = c(purple, slate)) +
theme_void() +
theme(legend.position = "none")
```

.col-left[.hi-slate[Validation (sub)set]]
.col-right[.hi-purple[Training set:] .purple[Model training]]

---

```{r, plot-validation-set-3, echo = F, dependson = "data-validation-set", fig.height = 3, cache = T}
ggplot(data = v_df, aes(x, y, fill = grp2, color = grp2)) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", slate)) +
scale_color_manual("", values = c(purple, slate)) +
theme_void() +
theme(legend.position = "none")
```

.col-left[.hi-slate[Validation (sub)set]]
.col-right[.hi-purple[Training set:] .purple[Model training]]

---
layout: true
# Resampling methods
## Option 1: The .it[validation set] approach

---
.ex[Example] We could use the validation-set approach to help select the degree of a polynomial for a linear-regression model.

--

The goal of the validation set is to .hi-pink[.it[estimate] out-of-sample (test) error.]

.qa[Q] So what?

--

- Estimates come with .b[uncertainty]—varying from sample to sample.

- Variability (standard errors) is larger with .b[smaller samples].

.qa[Problem] This estimated error is often based upon a fairly small sample (<30% of our training data). So its variance can be large.
---
exclude: true

```{r, sim-validation, include = F, cache = T}
# Generate population and sample
N = 1e5
set.seed(12345)
pop_dt = data.table(
  x1 = runif(N, min = -1, max = 1),
  x2 = runif(N, min = -1, max = 1),
  x3 = runif(N, min = -1, max = 1),
  er = rnorm(N, sd = 3)
)
pop_dt %<>% mutate(
  y = 3 + 5 * x1 - 4 * x2 + 3 * x1 * x2 * x3 + x3 - 2 * x3^2 + 0.1 * x3^3 + er
)
# Grab our sample
sample_dt = pop_dt[1:1e3,]
# For 10 seeds, grab validation set and estimate flexibility
vset_dt = lapply(
  X = 1:10,
  FUN = function(i) {
    # Set seed
    set.seed(i)
    # Grab validation set
    v_i = sample.int(1e3, size = 500, replace = F)
    vset_i = sample_dt[v_i,]
    tset_i = sample_dt[setdiff(1:1e3, v_i),]
    # Train models for y~x3 and grab their validation MSEs
    mse_i = lapply(
      X = 1:10,
      FUN = function(p) {
        # Train the model
        model_ip = lm(y ~ poly(x3, p, raw = T), data = tset_i)
        # Predict
        mean((vset_i$y - predict(model_ip, newdata = vset_i, se.fit = F))^2)
      }
    ) %>% unlist()
    # Create dataset
    data.table(iter = i, degree = 1:10, mse = mse_i)
  }
) %>% rbindlist()
# Repeat using full training model to train and full population to test
mse_true = lapply(
  X = 1:10,
  FUN = function(p) {
    # Train the model
    model_p = lm(y ~ poly(x3, p, raw = T), data = sample_dt)
    # Predict
    mean((pop_dt[-(1:1e3),]$y - predict(model_p, newdata = pop_dt[-(1:1e3),], se.fit = F))^2)
  }
) %>% unlist()
true_dt = data.table(degree = 1:10, mse = mse_true, iter = 1)
```


---
name: validation-simulation
layout: false
class: clear, middle

.b[Validation MSE] for 10 different validation samples
```{r, plot-vset-sim, echo = F, dependson = "sim-validation", cache = T}
ggplot(data = vset_dt, aes(x = degree, y = mse, color = iter, group = iter)) +
geom_line() +
geom_point(shape = 1) +
scale_x_continuous("Polynomial degree of x", breaks = seq(2, 10, 2)) +
ylab("Validation-set MSE") +
theme_minimal(base_size = 18, base_family = "Fira Sans Book") +
scale_color_viridis_c(option = "magma", begin = 0.3, end = 0.9) +
theme(legend.position = "none")
```
---
layout: false
class: clear, middle

.b[True test MSE] compared to validation-set estimates
```{r, plot-vset-sim-2, echo = F, dependson = "sim-validation", cache = T}
ggplot(data = vset_dt, aes(x = degree, y = mse, color = iter, group = iter)) +
geom_line() +
geom_point(shape = 1) +
geom_line(data = true_dt, aes(x = degree, y = mse), color = "black", size = 1) +
geom_point(data = true_dt, aes(x = degree, y = mse), color = "black", size = 3) +
scale_x_continuous("Polynomial degree of x", breaks = seq(2, 10, 2)) +
ylab("MSE") +
theme_minimal(base_size = 18, base_family = "Fira Sans Book") +
scale_color_viridis_c(option = "magma", begin = 0.3, end = 0.9) +
theme(legend.position = "none")
```

---
# Resampling methods
## Option 1: The .it[validation set] approach

The validation-set approach has two major drawbacks:

1. .hi[High variability.] The validation MSE is highly sensitive to exactly which observations are included in the validation set.
  - So validation MSE is a **noisy** estimator of test-set MSE.

2. .hi[Inefficiency in training our model.] We're throwing away the validation data when training the model.
  - The model isn't as good as it will be once we use all the data.
  - So validation MSE is also a **biased** estimate of test-set MSE.


---
layout: true
# Resampling methods
## Option 2: Leave-one-out cross validation

Each observation takes a turn as the .hi-slate[validation set],
<br>while the other n-1 observations get to .hi-purple[train the model].
<br>
<br>

---
exclude: true

```{r, data-loocv, include = F, cache = T}
# Generate data
X = 40
Y = 12
loocv_df = expand_grid(
  x = 1:X,
  y = -(1:Y)
) %>% mutate(
  i = 1:(X * Y),
  grp_1 = if_else(i == 1, "Validate", "Train"),
  grp_2 = if_else(i == 2, "Validate", "Train"),
  grp_3 = if_else(i == 3, "Validate", "Train"),
  grp_4 = if_else(i == 4, "Validate", "Train"),
  grp_5 = if_else(i == 5, "Validate", "Train"),
  grp_n = if_else(i == X*Y, "Validate", "Train")
)
```

---
```{r, plot-loocv-1, echo = F, fig.height = 3, dependson = "data-loocv", cache = T}
ggplot(data = loocv_df, aes(x, y, fill = grp_1, color = grp_1)) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", slate)) +
scale_color_manual("", values = c(purple, slate)) +
theme_void() +
theme(legend.position = "none")
```

.slate[Observation 1's turn for validation produces MSE.sub[1]].
---
```{r, plot-loocv-2, echo = F, fig.height = 3, dependson = "data-loocv", cache = T}
ggplot(data = loocv_df, aes(x, y, fill = grp_2, color = grp_2)) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", slate)) +
scale_color_manual("", values = c(purple, slate)) +
theme_void() +
theme(legend.position = "none")
```

.slate[Observation 2's turn for validation produces MSE.sub[2]].
---
```{r, plot-loocv-3, echo = F, fig.height = 3, dependson = "data-loocv", cache = T}
ggplot(data = loocv_df, aes(x, y, fill = grp_3, color = grp_3)) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", slate)) +
scale_color_manual("", values = c(purple, slate)) +
theme_void() +
theme(legend.position = "none")
```

.slate[Observation 3's turn for validation produces MSE.sub[3]].
---
```{r, plot-loocv-4, echo = F, fig.height = 3, dependson = "data-loocv", cache = T}
ggplot(data = loocv_df, aes(x, y, fill = grp_4, color = grp_4)) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", slate)) +
scale_color_manual("", values = c(purple, slate)) +
theme_void() +
theme(legend.position = "none")
```

.slate[Observation 4's turn for validation produces MSE.sub[4]].
---
```{r, plot-loocv-5, echo = F, fig.height = 3, dependson = "data-loocv", cache = T}
ggplot(data = loocv_df, aes(x, y, fill = grp_5, color = grp_5)) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", slate)) +
scale_color_manual("", values = c(purple, slate)) +
theme_void() +
theme(legend.position = "none")
```

.slate[Observation 5's turn for validation produces MSE.sub[5]].
---
```{r, plot-loocv-n, echo = F, fig.height = 3, dependson = "data-loocv", cache = T}
# The final observation
ggplot(data = loocv_df, aes(x, y, fill = grp_n, color = grp_n)) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", slate)) +
scale_color_manual("", values = c(purple, slate)) +
theme_void() +
theme(legend.position = "none")
```

.slate[Observation n's turn for validation produces MSE.sub[n]].
---
layout: true
# Resampling methods
## Option 2: Leave-one-out cross validation

---
Because .hi-pink[LOOCV uses n-1 observations] to train the model (and n-1 ≈ n), MSE.sub[i] is approximately unbiased for the test MSE.

.qa[Problem] MSE.sub[i] is a terribly noisy estimator for test MSE (albeit ≈unbiased).
--
<br>.qa[Solution] Take the mean!
$$
\begin{align}
  \text{CV}_{(n)} = \dfrac{1}{n} \sum_{i=1}^{n} \text{MSE}_i
\end{align}
$$
--

1. LOOCV .b[reduces bias] by using n-1 (almost all) observations for training.
2. LOOCV .b[resolves variance]: it uses all possible comparisons<br>(it doesn't depend on which validation-test split you make).

---
exclude: true

```{r, mse-loocv, include = F, cache = T, dependson = "sim-validation"}
# Calculate LOOCV MSE for each p
mse_loocv = lapply(
  X = 1:10,
  FUN = function(p) {
    # Train the model
    model_p = lm(y ~ poly(x3, p, raw = T), data = sample_dt)
    # Leverage
    h_p = hatvalues(model_p)
    # y and predictions
    y_p = sample_dt$y
    y_hat_p = model_p$fitted.values
    # MSE
    data.table(
      degree = p,
      mse = 1 / nrow(sample_dt) * sum(((y_p - y_hat_p) / (1 - h_p))^2),
      iter = 1
    )
  }
) %>% rbindlist()
```
---
name: ex-loocv
layout: false
class: clear, middle

.b[True test MSE] and .hi-orange[LOOCV MSE] compared to .hi-purple[validation-set estimates]
```{r, plot-loocv-mse, echo = F, dependson = "mse-loocv", cache = T}
ggplot(data = vset_dt, aes(x = degree, y = mse, group = iter)) +
geom_line(alpha = 0.35, color = purple) +
geom_point(alpha = 0.35, color = purple, shape = 1) +
geom_line(data = true_dt, aes(x = degree, y = mse), color = "black", size = 1) +
geom_point(data = true_dt, aes(x = degree, y = mse), color = "black", size = 3) +
geom_line(data = mse_loocv, aes(x = degree, y = mse), color = orange, size = 1) +
geom_point(data = mse_loocv, aes(x = degree, y = mse), color = orange, size = 3) +
scale_x_continuous("Polynomial degree of x", breaks = seq(2, 10, 2)) +
ylab("MSE") +
theme_minimal(base_size = 18, base_family = "Fira Sans Book") +
scale_color_viridis_c(option = "magma", begin = 0.3, end = 0.9) +
theme(legend.position = "none")
```
---
layout: true
# Resampling methods
## Best option: k-fold cross validation

---
name: resampling-kcv

Leave-one-out cross validation is a special case of a broader strategy:
<br>.hi[k-fold cross validation].

1. .b[Divide] the training data into $k$ equally sized groups (folds).
2. .b[Iterate] over the $k$ folds, treating each as a validation set once<br>(training the model on the other $k-1$ folds).
3. .b[Average] the folds' MSEs to estimate test MSE.

--

Benefits?
--

1. .b[Less computationally demanding] (fit model $k=$ 5 or 10 times; not $n$).
--

2. .b[Greater accuracy] (in general) due to bias-variance tradeoff!
  - Somewhat higher bias, relative to LOOCV: $n-1$ *vs.* $(k-1)/k$.
  - Lower variance due to high-degree of correlation in LOOCV MSE.sub[i].

---
exclude: true

```{r, data-cv, include = F, cache = T}
# Generate data
X = 40
Y = 12
set.seed(12345)
cv_df = expand_grid(
  x = 1:X,
  y = 1:Y
) %>% mutate(
  id = 1:(X*Y),
  grp = sample(X * Y) %% 5 + 1
)
# Find groups
a = seq(1, X*Y, by = X*Y/5)
b = c(a[-1] - 1, X*Y)
```

---
layout: true
# Resampling methods
## Best option: k-fold cross validation

With $k$-fold cross validation, we estimate test MSE as
$$
\begin{align}
  \text{CV}_{(k)} = \dfrac{1}{k} \sum_{i=1}^{k} \text{MSE}_{i}
\end{align}
$$
---

```{r, plot-cvk-0a, echo = F, fig.height = 3, dependson = "data-cv"}
ggplot(data = cv_df, aes(x, y, color = grp)) +
geom_point(size = 4.5) +
scale_color_viridis_c(option = "magma", end = 0.925) +
theme_void() +
theme(legend.position = "none")
```

Our $k=$ 5 folds.
---

```{r, plot-cvk-0b, echo = F, fig.height = 3, dependson = "data-cv"}
ggplot(data = cv_df, aes(x, y, color = grp == 1, fill = grp == 1)) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", slate)) +
scale_color_manual("", values = c(purple, slate)) +
theme_void() +
theme(legend.position = "none")
```

Each fold takes a turn at .hi-slate[validation]. The other $k-1$ folds .hi-purple[train].
---

```{r, plot-cvk-1, echo = F, fig.height = 3, dependson = "data-cv"}
ggplot(
  data = cv_df,
  aes(x, y, color = between(id, a[1], b[1]), fill = between(id, a[1], b[1]))
) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", slate)) +
scale_color_manual("", values = c(purple, slate)) +
theme_void() +
theme(legend.position = "none")
```

For $k=5$, fold number $1$ as the .hi-slate[validation set] produces MSE.sub[k=1].
---

```{r, plot-cvk-2, echo = F, fig.height = 3, dependson = "data-cv"}
ggplot(
  data = cv_df,
  aes(x, y, color = between(id, a[2], b[2]), fill = between(id, a[2], b[2]))
) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", slate)) +
scale_color_manual("", values = c(purple, slate)) +
theme_void() +
theme(legend.position = "none")
```

For $k=5$, fold number $2$ as the .hi-slate[validation set] produces MSE.sub[k=2].
---

```{r, plot-cvk-3, echo = F, fig.height = 3, dependson = "data-cv"}
ggplot(
  data = cv_df,
  aes(x, y, color = between(id, a[3], b[3]), fill = between(id, a[3], b[3]))
) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", slate)) +
scale_color_manual("", values = c(purple, slate)) +
theme_void() +
theme(legend.position = "none")
```

For $k=5$, fold number $3$ as the .hi-slate[validation set] produces MSE.sub[k=3].
---

```{r, plot-cvk-4, echo = F, fig.height = 3, dependson = "data-cv"}
ggplot(
  data = cv_df,
  aes(x, y, color = between(id, a[4], b[4]), fill = between(id, a[4], b[4]))
) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", slate)) +
scale_color_manual("", values = c(purple, slate)) +
theme_void() +
theme(legend.position = "none")
```

For $k=5$, fold number $4$ as the .hi-slate[validation set] produces MSE.sub[k=4].
---

```{r, plot-cvk-5, echo = F, fig.height = 3, dependson = "data-cv"}
ggplot(
  data = cv_df,
  aes(x, y, color = between(id, a[5], b[5]), fill = between(id, a[5], b[5]))
) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", slate)) +
scale_color_manual("", values = c(purple, slate)) +
theme_void() +
theme(legend.position = "none")
```

For $k=5$, fold number $5$ as the .hi-slate[validation set] produces MSE.sub[k=5].
---
exclude: true

```{r, sim-cvk, include = F, cache = T, dependson = "sim-validation"}
# 5-fold cross validation, 20 times
cv_sim = lapply(X = 1:20, FUN = function(s) {
  set.seed(s)
  # Assign folds for CV
  sample_cv = copy(sample_dt) %T>% setDT()
  sample_cv[, fold := sample(1:.N) %% 5 + 1]
  # Iterate over polynomial degrees
  mse_s = lapply(X = 1:10, function(p) {
    # Iterate over folds
    lapply(X = 1:5, FUN = function(k) {
      # Train the model
      model_spk = lm(y ~ poly(x3, p, raw = T), data = sample_cv[fold != k])
      # Predict
      mean(
        (sample_cv[fold == k,y] - predict(
          model_spk,
          newdata = sample_cv[fold == k],
          se.fit = F
        )
      )^2)
    }) %>% unlist() %>% mean()
  }) %>% unlist()
  data.table(degree = 1:10, mse = mse_s, iter = s)
}) %>% rbindlist()
```
---
name: ex-cv-sim
layout: false
class: clear, middle

.b[Test MSE] .it[vs.] estimates: .orange[LOOCV], .pink[5-fold CV] (20x), and .purple[validation set] (10x)
```{r, plot-cv-mse, echo = F, dependson = c("sim-validation", "mse-loocv", "sim-cvk"), cache = T}
ggplot(data = vset_dt, aes(x = degree, y = mse, group = iter)) +
geom_line(alpha = 0.5, color = purple) +
geom_point(alpha = 0.5, color = purple, shape = 1) +
geom_line(data = true_dt, aes(x = degree, y = mse), color = "black", size = 1) +
geom_point(data = true_dt, aes(x = degree, y = mse), color = "black", size = 3) +
geom_line(data = cv_sim, aes(x = degree, y = mse, group = iter), color = red_pink, size = 1) +
geom_point(data = cv_sim, aes(x = degree, y = mse, group = iter), color = red_pink, size = 3) +
geom_line(data = mse_loocv, aes(x = degree, y = mse), color = orange, size = 1) +
geom_point(data = mse_loocv, aes(x = degree, y = mse), color = orange, size = 3) +
scale_x_continuous("Polynomial degree of x", breaks = seq(2, 10, 2)) +
ylab("MSE") +
theme_minimal(base_size = 18, base_family = "Fira Sans Book") +
scale_color_viridis_c(option = "magma", begin = 0.3, end = 0.9) +
theme(legend.position = "none")
```

---
name: holdout-caveats
layout: false
# Resampling methods
## Caveat

So far, we've treated each observation as independent of each other observation. But exceptions are everywhere:
- Individuals in a household (cross-sectional data)
- Days of stock returns (time series data)
- Observations of the same county over time (panel data)
- Pixels of satellite imagery (spatial data)

For cases like these, we need to modify the cross-validation procedures to take **dependence** into account.
- We'll come back to methods for doing this soon.


---
# Summary

.smaller[
.pull-left[
[**Prediction vs. causal inference**](#goal)
- In causal inference we want to estimate the treatment effect $\hat{\beta}$.
- In prediction problems we want to estimate the outcome value $\hat{Y_i}$.

**[Statistical learning](#learning)**
- Supervised vs. unsupervised learning.
- Regression vs. classification.
- Reducible vs. irreducible error.
]
.pull-right[
**[Model accuracy](#accuracy)**
- Models can be assessed using loss functions that combine prediction errors.
- For regression problems, MSE is the most common loss function.
- Using separate testing and training data avoids overfitting.

**[Cross-validation](#resampling)**
- Resampling methods avoid overfitting in model assessment and selection.
- Validation set approach.
- Leave-one-out cross-validation.
- *k*-fold cross-validation.
]
]
